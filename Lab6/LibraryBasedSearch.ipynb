{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Wyszukiwarka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Plan:\n",
    "tremin: 20.12.2017\n",
    "1. Dictionary\n",
    "    - Stop words removed\n",
    "    - Stemming (Porter Stemmer, Porter Stemmer 2)\n",
    "    - Reduction (optional)\n",
    "2. Budowa BackOfWords (indeksowanie)\n",
    "    - documentTermMatrix (wektory słów dla każdego artykułu)\n",
    "    - Inverse Document Frequency (IDF(w) - przemnożenie kolumny słowa w w documentTermMatrix przez IDF(w)\n",
    "    - Normalizacja (sprowadzenie wektora dla każdego dokumentu do jednostkowego - przydatne przy danych o zróżnicowanej długości)\n",
    "3. Query\n",
    "    - Przekleństwo wymiaru\n",
    "    - iloczyn skalarny\n",
    "4. SVD (LRMA)\n",
    "    - biblioteka, która liczy pierwsze k wektorów \n",
    "    - biblioteka dla danych żadkich\n",
    "5. * Semantyka w nietrywialny sposób\n",
    "    - Latent semantic indexing\n",
    "    - Latent Dirichlet approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Drop non english articles to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3933e0efd9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdrop_non_en\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from langdetect import detect, lang_detect_exception\n",
    "\n",
    "\n",
    "def drop_non_en(path):\n",
    "    \"\"\"Drop.\"\"\"\n",
    "    with open(path, encoding='utf-8', errors=\"ignore\") as csvfile:\n",
    "        with open('./droped.csv', 'w') as outfile:\n",
    "            writer = csv.writer(outfile)\n",
    "            reader = csv.reader(csvfile)\n",
    "            writer.writerow(next(reader))\n",
    "            for row in tqdm(reader):\n",
    "                try:\n",
    "                    if detect(row[4]) == 'en':\n",
    "                        writer.writerow(row)\n",
    "                except lang_detect_exception.LangDetectException:\n",
    "                    pass\n",
    "\n",
    "\n",
    "drop_non_en(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer, snowball\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix, lil_matrix, diags\n",
    "from scipy.sparse.linalg import svds\n",
    "import scipy\n",
    "import itertools\n",
    "from math import log\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filename = \"data.tsv\"\n",
    "# filename = \"articles2.csv\"\n",
    "filename = 'PythonApp/buzzfeed.csv'\n",
    "stopwords_filename = \"stopwords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if filename == 'PythonApp/buzzfeed.csv':\n",
    "    column_name = 'article'\n",
    "    texts = pd.read_csv(filename)\n",
    "    texts = texts.fillna('')\n",
    "    texts['title'].add_suffix(' ')\n",
    "    texts[column_name] = texts['title'] + \" \" + texts['first_paragraph'] + \" \" + texts['text']\n",
    "elif filename == 'articles2.csv':\n",
    "    texts = pd.read_csv(filename)\n",
    "    column_name = 'Article'\n",
    "elif filename == 'data.tsv':\n",
    "    texts = pd.read_csv(filename, header=0, delimiter=\"\\t\")\n",
    "    column_name = 'review'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36909, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(stopwords_filename, \"r\") as file:\n",
    "    stopwords = set(file.read().splitlines()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def parse_words(tokens, ps, wnl):\n",
    "    for t in tokens:\n",
    "        t = t.lower()\n",
    "        t = wnl.lemmatize(t)\n",
    "        t = re.sub('[^a-z]', '', t)\n",
    "        t = ps.stem(t)\n",
    "        yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CustomVectorizer(TfidfVectorizer):\n",
    "    \"\"\"TfidfVectorizer is a CountVectorizer with IDF transformation and normalisation:\n",
    "        vectorizer = CustomVectorizer()\n",
    "        vectorizer.fit_transform(texts)\n",
    "        transformer = TfidfTransformer()\n",
    "        tfidf = transformer.fit_transform(X)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, stop_words='english'):\n",
    "        super(CustomVectorizer, self).__init__(stop_words=stop_words)\n",
    "#         ps = PorterStemmer()\n",
    "        self.ps = snowball.EnglishStemmer()\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def build_tokenizer(self):\n",
    "        tokenize = super(CustomVectorizer, self).build_tokenizer()\n",
    "        return lambda doc: list(parse_words(tokenize(doc), self.ps, self.wnl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CustomVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = texts.head(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [02:45<00:00, 90.71it/s] \n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vectorizer.fit_transform(tqdm(texts[column_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# SVD AND LOW RANK APPROXIMATION FOR DATA MATRIX\n",
    "U, s, V = svds(bag_of_words)\n",
    "#print(U.shape, V.shape, s.shape)\n",
    "\n",
    "approx = round(0.3)\n",
    "lra = len(s)*approx\n",
    "s[len(s) - lra:] = [0] * lra\n",
    "\n",
    "S = diags(s)\n",
    "\n",
    "bag_of_words_svd = U @ (S @ V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Save model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('PythonApp/bag_of_words', bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.save('PythonApp/bag_of_words_svd', bag_of_words_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PythonApp/vectorizer.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, 'PythonApp/vectorizer.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bag_of_words = load_sparse_csr('PythonApp/bag_of_words.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('PythonApp/vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "bag_of_words_svd = np.load('PythonApp/bag_of_words_svd.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def count_non_zero(bag_of_words):\n",
    "    if type(bag_of_words) == scipy.sparse.csr.csr_matrix:\n",
    "        return bag_of_words.count_nonzero()\n",
    "    else:\n",
    "        return np.count_nonzero(bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def get_best_result(query, bag_of_words, k=10):\n",
    "#     query_vec = vectorizer.transform([query])[0]\n",
    "#     query_length = query_vec.count_nonzero()\n",
    "#     number_of_docs = len(texts)\n",
    "#     matches = {i: 0 for i in range(number_of_docs)}\n",
    "#     for i in tqdm(range(number_of_docs)):\n",
    "#         matches[i] = query_vec.multiply(bag_of_words[i]).sum()/(query_length * count_non_zero(bag_of_words[i]))\n",
    "# #         matches[i] = [word * backOfWords[i,j] for j,word in enumerate(query_vec.nonzero()[1])][0].sum()/(query_length * backOfWords[i].count_nonzero())\n",
    "#     return list(dict(sorted(matches.items(), key=lambda x: x[1], reverse=True)))[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_best_result(query, bag_of_words, k=10):\n",
    "    query_vec = vectorizer.transform([query])[0]\n",
    "    query_length = query_vec.count_nonzero()\n",
    "    number_of_docs = texts.shape[0]\n",
    "    matches = dict()\n",
    "    for i in tqdm(range(number_of_docs)):\n",
    "        matches[i] = 0\n",
    "        for word_index in query_vec.indices:\n",
    "            matches[i] += bag_of_words[i, word_index]\n",
    "        matches[i] /= (query_length * count_non_zero(bag_of_words[i]))\n",
    "    return list(dict(heapq.nlargest(k, matches.items(), key=lambda x: x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_urls(best_results):\n",
    "    best_results = list(map(lambda x: texts['url'][x], best_results))\n",
    "    for i, result in enumerate(best_results):\n",
    "        print(i, ': ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:04<00:00, 3441.27it/s]\n"
     ]
    }
   ],
   "source": [
    "best_results = get_best_result(\"Trump becomes the president\", bag_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5313, 11703, 1586, 5633, 11800, 4703, 14267, 878, 6419, 2409]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  https://www.buzzfeed.com/sophmason/how-donald-is-your-trump-2t6pk\n",
      "1 :  https://www.buzzfeed.com/luxuryhd/trumps-luxury-hotels-top-10-aa-1vd0x\n",
      "2 :  https://www.buzzfeed.com/wooishgurl7/who-said-it-donald-trump-or-michael-scott-2g7qh\n",
      "3 :  https://www.buzzfeed.com/deenazaidi/immigration-marine-le-pen-and-the-trump-connecti-2hg33\n",
      "4 :  https://www.buzzfeed.com/kevinjamesshay/ode-to-mr-trump-1wwul\n",
      "5 :  https://www.buzzfeed.com/leonardos415b01eb6/i-was-inspired-by-trump-success-2t2br\n",
      "6 :  https://www.buzzfeed.com/cinamatics/top-5-things-you-may-not-know-about-donald-trump-2zrei\n",
      "7 :  https://www.buzzfeed.com/katiejoyxox/the-trump-transition-1hg4y\n",
      "8 :  https://www.buzzfeed.com/catonb/did-donald-trump-really-say-this-or-did-bugs-bunny-snyg\n",
      "9 :  https://www.buzzfeed.com/mallorymuratore/who-said-it-donald-trump-or-corinne-from-the-bach-nwve\n"
     ]
    }
   ],
   "source": [
    "print_urls(best_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:04<00:00, 3400.34it/s]\n"
     ]
    }
   ],
   "source": [
    "best_results = get_best_result(\"Trump becomes the president\", bag_of_words_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1339, 13496, 12989, 13309, 11504, 3252, 6339, 6259, 4707, 6485]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  https://www.buzzfeed.com/annehelenpetersen/why-donald-trump-didnt-get-elected-the-first-time-around\n",
      "1 :  https://www.buzzfeed.com/jimdalrympleii/trump-highly-classified-information-russians\n",
      "2 :  https://www.buzzfeed.com/buzzfeednews/trump-first-foreign-trip\n",
      "3 :  https://www.buzzfeed.com/tariniparti/exhausted-republicans-are-starting-to-get-fed-up-with-the\n",
      "4 :  https://www.buzzfeed.com/henrygomez/republicans-are-waiting-for-trumps-help-or-wrath-as-key\n",
      "5 :  https://www.buzzfeed.com/jimwaterson/theresa-may-won-trump-over-but-the-special-relationship-is-n\n",
      "6 :  https://www.buzzfeed.com/jimwaterson/british-mp-says-uk-is-pimping-out-the-queen-for-donald-trump\n",
      "7 :  https://www.buzzfeed.com/buzzfeednews/whats-going-on-around-the-world-today-feb-17-17\n",
      "8 :  https://www.buzzfeed.com/tariniparti/trump-world-including-steve-bannon-is-already-looking-at-the\n",
      "9 :  https://www.buzzfeed.com/tomnamako/donald-trump-presser-media\n"
     ]
    }
   ],
   "source": [
    "print_urls(best_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
